<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>time包的Ticker</title>
      <link href="/2021/08/21/notes/time-package-ticker/"/>
      <url>/2021/08/21/notes/time-package-ticker/</url>
      
        <content type="html"><![CDATA[<h3 id="Ticker结构体"><a href="#Ticker结构体" class="headerlink" title="Ticker结构体"></a>Ticker结构体</h3><p>一个Ticker拥有一个通道，每隔一段时间传递一次<code>tick</code></p><pre><code class="go">type Ticker struct &#123;    C &lt;-chan Time // 周期性传递时间信息的通道。    r runtimeTimer&#125;</code></pre><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ul><li><p><code>NewTicker(d Duration) *Ticker</code></p><p>NewTicker 返回一个新的 Ticker，其中包含一个通道，该通道将在每个滴答后发送通道上的时间。<br>滴答的周期由持续时间参数指定。<br>持续时间 d 必须大于零； 如果没有，NewTicker 会恐慌。<br>停止自动收报机以释放相关资源。</p></li><li><p><code>Tick(d Duration) &lt;-chan Time</code></p><p>Tick 是 NewTicker 的便捷包装器，仅提供对滴答通道的访问。<br>尽管 Tick 对不需要关闭 Ticker 的客户端很有用，但请注意，如果无法关闭它，则垃圾收集器无法恢复底层的 Ticker； 它“泄漏”。<br>与 NewTicker 不同，如果 d &lt;= 0，Tick 将返回 nil。</p></li></ul><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><ul><li><p><code>Stop()</code></p><p>  停止关闭<code>ticker</code><br>  停止后，将不再发送<code>ticks</code><br>停止不会关闭通道，以防止从通道读取的并发 goroutine 看到错误的<code>tick</code>。</p></li><li><p><code>Reset(d Duration)</code></p><p>重置停止<code>ticker</code>并将其周期重置为指定的持续时间。<br>下一个<code>tick</code>将在新周期结束后到达。</p></li></ul><h3 id="定时任务-以60秒为例-NewTicker"><a href="#定时任务-以60秒为例-NewTicker" class="headerlink" title="定时任务(以60秒为例)NewTicker"></a>定时任务(以60秒为例)NewTicker</h3><pre><code class="go">ticker := time.NewTicker(time.Second * 60)for &#123;    select &#123;    case &lt;-ticker.C:        fmt.Println(&quot;a&quot;)    &#125;&#125;</code></pre><h3 id="定时任务-以60秒为例-Tick"><a href="#定时任务-以60秒为例-Tick" class="headerlink" title="定时任务(以60秒为例)Tick"></a>定时任务(以60秒为例)Tick</h3><pre><code class="go">for range time.Tick(time.Second * 60) &#123;    fmt.Println(&quot;a&quot;)&#125;</code></pre><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href=""></a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Golang </tag>
            
            <tag> Ticker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>base32编码解码</title>
      <link href="/2021/08/14/notes/base32-bian-ma-jie-ma/"/>
      <url>/2021/08/14/notes/base32-bian-ma-jie-ma/</url>
      
        <content type="html"><![CDATA[<h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><ul><li>StdPadding：使用<code>rune</code>类型的<code>=</code>作为标准填充字符。</li><li>NoPadding：-1 无填充。</li></ul><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>RFC 4648定义的标准base32编码。</p><pre><code class="go">var StdEncoding = NewEncoding(encodeStd)</code></pre><p>RFC 4648定义的`Extended Hex Alphabet’一般用于DNS</p><pre><code class="go">var HexEncoding = NewEncoding(encodeHex)</code></pre><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ul><li><p>NewEncoding(encoder string) *Encoding</p></li><li><p>NewEncoder(enc *Encoding, w io.Writer) io.WriteCloser</p></li><li><p>NewDecoder(enc *Encoding, r io.Reader) io.Reader</p></li></ul><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><ul><li>Encode(dst, src []byte)</li><li>EncodeToString(src []byte) string</li><li>EncodedLen(n int) int</li><li>Decode(dst, src []byte) (n int, err error)</li><li>DecodeString(s string) ([]byte, error)</li><li>DecodedLen(n int) int</li></ul><h2 id="编码解码的三种方式"><a href="#编码解码的三种方式" class="headerlink" title="编码解码的三种方式"></a>编码解码的三种方式</h2><h3 id="使用NewEncoder-与NewDecoder"><a href="#使用NewEncoder-与NewDecoder" class="headerlink" title="使用NewEncoder()与NewDecoder()"></a>使用NewEncoder()与NewDecoder()</h3><pre><code class="go">//编码buffer := bytes.Buffer&#123;&#125;encoder := base32.NewEncoder(base32.StdEncoding, &amp;buffer)defer encoder.Close()encoder.Write([]byte(&quot;hello NewEncoder&quot;))//解码decodeBuf := make([]byte, base32.StdEncoding.DecodedLen(len(buffer.Bytes())))decoder := base32.NewDecoder(base32.StdEncoding, &amp;buffer)decoder.Read(decodeBuf)</code></pre><h3 id="使用EncodeToString-和DecodeString"><a href="#使用EncodeToString-和DecodeString" class="headerlink" title="使用EncodeToString()和DecodeString()"></a>使用EncodeToString()和DecodeString()</h3><pre><code class="go">//编码encoder := base32.StdEncoding.EncodeToString([]byte(&quot;hello EncodeToString&quot;)//解码decoder, err := base32.StdEncoding.DecodeString(encoder)          </code></pre><h3 id="使用Encode-和Decode"><a href="#使用Encode-和Decode" class="headerlink" title="使用Encode()和Decode()"></a>使用Encode()和Decode()</h3><pre><code class="go">//编码data := &quot;hello encode&quot;buf := make([]byte, base32.StdEncoding.EncodedLen(len(data)))base32.StdEncoding.Encode(buf, []byte(data))//解码debuf := make([]byte, base32.StdEncoding.DecodedLen(len(buf)))n, err := base32.StdEncoding.Decode(debuf, buf)fmt.Println(string(debuf[:n]))</code></pre><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://github.com/golang/go/blob/7eaabae84d8b69216356b84ebc7c86917100f99a/src/encoding/base32/base32.go#L6">官方base32地址</a></p><p><a href="https://www.rfc-editor.org/rfc/inline-errata/rfc4648.html">RFC 4648</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> base32 </tag>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TCMalloc : Thread-Caching Malloc【译】</title>
      <link href="/2021/08/09/trans/tcmalloc-thread-caching-malloc/"/>
      <url>/2021/08/09/trans/tcmalloc-thread-caching-malloc/</url>
      
        <content type="html"><![CDATA[<h2 id="TCMalloc"><a href="#TCMalloc" class="headerlink" title="TCMalloc"></a>TCMalloc</h2><ul><li><a href="#%E5%8A%A8%E6%9C%BA">动机</a></li><li><a href="#%E7%94%A8%E6%B3%95">用法</a></li><li><a href="#%E6%A6%82%E8%BF%B0">概述</a></li><li><a href="#%E5%B0%8F%E5%AF%B9%E8%B1%A1%E5%88%86%E9%85%8D">小对象分配</a></li><li><a href="#%E5%A4%A7%E5%AF%B9%E8%B1%A1%E5%88%86%E9%85%8D">大对象分配</a></li><li><a href="#Spans">Spans</a></li><li><a href="#%E8%A7%A3%E9%99%A4%E5%88%86%E9%85%8D">解除分配</a></li><li><a href="#%E5%B0%8F%E5%AF%B9%E8%B1%A1%E7%9A%84%E4%B8%AD%E5%A4%AE%E7%A9%BA%E9%97%B2%E5%88%97%E8%A1%A8">小对象的中央空闲列表</a></li><li><a href="#%E7%BA%BF%E7%A8%8B%E7%BC%93%E5%AD%98%E7%9A%84%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86">线程缓存的垃圾收集</a></li><li><a href="#%E6%80%A7%E8%83%BD%E8%AF%B4%E6%98%8E">性能说明</a><ul><li><a href="#PTMalloc2%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95">PTMalloc2单元测试</a></li></ul></li><li><a href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">注意事项</a></li></ul><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><h3 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h3><p>​    TCMalloc is faster than the glibc 2.3 malloc (available as a separate library called ptmalloc2) and other mallocs that I have tested. ptmalloc2 takes approximately 300 nanoseconds to execute a malloc/free pair on a 2.8 GHz P4 (for small objects). The TCMalloc implementation takes approximately 50 nanoseconds for the same operation pair. Speed is important for a malloc implementation because if malloc is not fast enough, application writers are inclined to write their own custom free lists on top of malloc. This can lead to extra complexity, and more memory usage unless the application writer is very careful to appropriately size the free lists and scavenge idle objects out of the free list</p><p>​    TCMalloc also reduces lock contention for multi-threaded programs. For small objects, there is virtually zero contention. For large objects, TCMalloc tries to use fine grained and efficient spinlocks. ptmalloc2 also reduces lock contention by using per-thread arenas but there is a big problem with ptmalloc2’s use of per-thread arenas. In ptmalloc2 memory can never move from one arena to another. This can lead to huge amounts of wasted space. For example, in one Google application, the first phase would allocate approximately 300MB of memory for its data structures. When the first phase finished, a second phase would be started in the same address space. If this second phase was assigned a different arena than the one used by the first phase, this phase would not reuse any of the memory left after the first phase and would add another 300MB to the address space. Similar memory blowup problems were also noticed in other applications.</p><p>​    Another benefit of TCMalloc is space-efficient representation of small objects. For example, N 8-byte objects can be allocated while using space approximately <code>8N * 1.01</code> bytes. I.e., a one-percent space overhead. ptmalloc2 uses a four-byte header for each object and (I think) rounds up the size to a multiple of 8 bytes and ends up using <code>16N</code> bytes.</p><h3 id="译"><a href="#译" class="headerlink" title="译"></a>译</h3><p>​    TCMalloc比glibc2.3 malloc（作为一个名为ptmalloc2的单独库提供）和我测试过的其他malloc更快。ptmalloc2在2.8GHz P4（用于小对象）上执行一次malloc/free对大约需要300纳秒。对于相同的操作，TCMalloc实现大约需要50纳秒。对于malloc的实现来说速度是非常重要的，因为如果 malloc 不够快，应用程序编写者倾向于在 malloc 之上编写他们自己的自定义空闲列表。这会导致额外的复杂性和更多的内存使用，除非应用程序编写者非常小心地适当调整空闲列表的大小并从空闲列表中清除空闲对象。</p><p>​    TCMalloc还减少了多线程程序的锁争用。对于小对象，几乎为零争用。对于大对象，TCMalloc尝试使用细粒度的和高效的自旋锁。ptmalloc2 还通过使用每线程 域（arenas） 来减少锁争用，但是 ptmalloc2 使用每线程 域（arenas）存在一个大问题。在ptmalloc2中，内存永远无法从一个域（arena）移动到另一个域（arena）。这可能会导致大量空间浪费。例如，在一个 Google 应用程序中，第一阶段将为其数据结构分配大约 300MB 的内存。当第一个阶段完成时，第二个阶段将在相同的地址空间中开始。如果第二阶段被分配到一个不同于第一阶段使用的域（arena），这个阶段将不会重用第一阶段之后留下的任何内存，并将另外 300MB 添加到地址空间。在其他应用程序中也发现了类似的内存爆炸问题。</p><p>​    TCMalloc 的另一个好处是小对象的空间高效表示。例如，可以分配 N 个 8 字节的对象总共需要 <code>8N*1.01</code>字节的空间。 即，百分之一的空间开销。ptmalloc2 为每个对象使用一个四字节的标头，并且（我认为）将大小四舍五入为 8 字节的倍数，最终使用<code>16N</code>字节。</p><h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><h3 id="原文-1"><a href="#原文-1" class="headerlink" title="原文"></a>原文</h3><p>​    To use TCmalloc, just link tcmalloc into your application via the “-ltcmalloc” linker flag.</p><p>​    You can use tcmalloc in applications you didn’t compile yourself, by using LD_PRELOAD:</p><pre><code>   $ LD_PRELOAD=&quot;/usr/lib/libtcmalloc.so&quot; </code></pre><p>​    LD_PRELOAD is tricky, and we don’t necessarily recommend this mode of usage.</p><p>​    TCMalloc includes a <a href="http://goog-perftools.sourceforge.net/doc/heap_checker.html">heap checker</a> and <a href="http://goog-perftools.sourceforge.net/doc/heap_profiler.html">heap profiler</a> as well.</p><p>​    If you’d rather link in a version of TCMalloc that does not include the heap profiler and checker (perhaps to reduce binary size for a static binary), you can link in <code>libtcmalloc_minimal</code> instead.</p><h3 id="译-1"><a href="#译-1" class="headerlink" title="译"></a>译</h3><p>​    要使用 TCmalloc，只需通过“-ltcmalloc”链接器标志将 tcmalloc 链接到您的应用程序。</p><p>​    您可以在不是自己编译的应用程序中使用 tcmalloc，方法是使用 LD_PRELOAD：</p><pre><code>   $ LD_PRELOAD=&quot;/usr/lib/libtcmalloc.so&quot; </code></pre><p>​    LD_PRELOAD 很棘手，我们不一定推荐这种使用模式。</p><p>​    TCMalloc 还包括一个堆检查器和堆分析器。</p><p>​    如果您更愿意链接不包含堆分析器和检查器的 TCMalloc 版本（可能是为了减少静态二进制文件的二进制大小），您可以使用链接<code>libtcmalloc_minimal</code>代替。</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="原文-2"><a href="#原文-2" class="headerlink" title="原文"></a>原文</h3><p>​    TCMalloc assigns each thread a thread-local cache. Small allocations are satisfied from the thread-local cache. Objects are moved from central data structures into a thread-local cache as needed, and periodic garbage collections are used to migrate memory back from a thread-local cache into the central data structures.</p><p><img src="/img/trans/overview.gif"></p><p>​    TCMalloc treates objects with size &lt;= 32K (“small” objects) differently from larger objects. Large objects are allocated directly from the central heap using a page-level allocator (a page is a 4K aligned region of memory). I.e., a large object is always page-aligned and occupies an integral number of pages.</p><p>​    A run of pages can be carved up into a sequence of small objects, each equally sized. For example a run of one page (4K) can be carved up into 32 objects of size 128 bytes each.</p><h3 id="译-2"><a href="#译-2" class="headerlink" title="译"></a>译</h3><p>​    TCMalloc为每个线程分配一个线程本地缓存。小对象的分配直接走线程本地缓存。根据需要，对象从中央数据结构移动到线程本地缓存中，并使用定期垃圾收集将内存从线程本地缓存迁移回中央数据结构。</p><p>​    TCMalloc 将size &lt;= 32K 的对象作为小对象，大于32K的对象为大对象。大对象使用页面级分配器直接从中央堆分配（页面是 4K 对齐的内存区域）。即，一个大对象总是页面对齐并占据整数页。</p><p>​    一连串的页面可以被分割成一系列大小相同的小对象。例如，一页 (4K) 的页面可以分成 32 个大小为 128 字节的对象。</p><h2 id="小对象分配"><a href="#小对象分配" class="headerlink" title="小对象分配"></a>小对象分配</h2><h3 id="原文-3"><a href="#原文-3" class="headerlink" title="原文"></a>原文</h3><p>Each small object size maps to one of approximately 170 allocatable size-classes. For example, all allocations in the range 961 to 1024 bytes are rounded up to 1024. The size-classes are spaced so that small sizes are separated by 8 bytes, larger sizes by 16 bytes, even larger sizes by 32 bytes, and so forth. The maximal spacing (for sizes &gt;= ~2K) is 256 bytes.</p><p>A thread cache contains a singly linked list of free objects per size-class.</p><p><img src="/img/trans/threadheap.gif"></p><p>When allocating a small object: (1) We map its size to the corresponding size-class. (2) Look in the corresponding free list in the thread cache for the current thread. (3) If the free list is not empty, we remove the first object from the list and return it. When following this fast path, TCMalloc acquires no locks at all. This helps speed-up allocation significantly because a lock/unlock pair takes approximately 100 nanoseconds on a 2.8 GHz Xeon.</p><p>If the free list is empty: (1) We fetch a bunch of objects from a central free list for this size-class (the central free list is shared by all threads). (2) Place them in the thread-local free list. (3) Return one of the newly fetched objects to the applications.</p><p>If the central free list is also empty: (1) We allocate a run of pages from the central page allocator. (2) Split the run into a set of objects of this size-class. (3) Place the new objects on the central free list. (4) As before, move some of these objects to the thread-local free list.</p><h3 id="译-3"><a href="#译-3" class="headerlink" title="译"></a>译</h3><p>​    每个小对象的字节数被映射到大约 170 个可分配的 size-class 中的一个。例如，961 到 1024 字节范围内的所有分配都会向上舍入为 1024。 size-class 被隔开，以便小尺寸由 8 个字节分隔，较大尺寸由 16 个字节分隔，更大尺寸由 32 个字节分隔，依此类推。最大间距（大小 &gt;= ~2K）为 256 字节。</p><p>​    线程缓存包含每个 size-class 的空闲对象的单向链接列表。</p><p>​    分配小对象时：</p><ul><li><p>我们将其大小映射到相应的 size-class。</p></li><li><p>在线程缓存中查找当前线程对应的空闲列表。</p></li><li><p>如果空闲列表不为空，我们从列表中删除第一个对象并返回它。</p><p>当我们按照这种方式分配时，TCMalloc不需要任何锁。这就可以极大提高分配的速度，因为锁/解锁操作在一个2.8GHz Xeon上大约需要100纳秒的时间。    </p><p>如果空闲列表为空：</p></li><li><p>我们从这个 size-class 的中央空闲列表中获取一堆对象（中央空闲列表由所有线程共享）。</p></li><li><p>将它们放入线程局部空闲列表中。</p></li><li><p>将新获取的对象之一返回给应用程序。</p><p>如果中央空闲列表也为空：</p></li><li><p>我们从中央页面分配器分配一系列页面。</p></li><li><p>切分这些页放在对应的size-class的集合中。</p></li><li><p>将新对象放在中央空闲列表上。</p></li><li><p>和以前一样，将这些对象中的一些移动到线程本地空闲列表中。</p></li></ul><h2 id="大对象分配"><a href="#大对象分配" class="headerlink" title="大对象分配"></a>大对象分配</h2><h3 id="原文-4"><a href="#原文-4" class="headerlink" title="原文"></a>原文</h3><p>​    A large object size (&gt; 32K) is rounded up to a page size (4K) and is handled by a central page heap. The central page heap is again an array of free lists. For <code>i &lt; 256</code>, the <code>k</code>th entry is a free list of runs that consist of <code>k</code> pages. The <code>256</code>th entry is a free list of runs that have length <code>&gt;= 256</code> pages:</p><p><img src="/img/trans/pageheap.gif"></p><p>​    An allocation for <code>k</code> pages is satisfied by looking in the <code>k</code>th free list. If that free list is empty, we look in the next free list, and so forth. Eventually, we look in the last free list if necessary. If that fails, we fetch memory from the system (using sbrk, mmap, or by mapping in portions of /dev/mem).</p><p>​    If an allocation for <code>k</code> pages is satisfied by a run of pages of length &gt; <code>k</code>, the remainder of the run is re-inserted back into the appropriate free list in the page heap.</p><h3 id="译-4"><a href="#译-4" class="headerlink" title="译"></a>译</h3><p>​    一个大对象的字节数（大于32k）按页（4k）对齐在中央页堆。中心页堆又是一个空闲列表数组。对于i&lt;256，第k项是一个包好k页的自由链表。但是第256项的自由链表的元素的内存是大于等于256页的字节数。</p><p>​    分配k页内存优先从第k个自由链表查找。如果这个自由链表为空，我会从下一个自由链表中找，如果也是空，继续找下一个。如果需要，我们会遍历到最后一个自由链表。如果失败了，我们直接向系统申请内存（使用sbrk、mmap、或者映射到 /dev/mem）。</p><p>​    如果在申请分配k个页时候实际分配的内存页大于k,剩余的页会被放回对应的页堆的自由链表中。</p><h2 id="Spans"><a href="#Spans" class="headerlink" title="Spans"></a>Spans</h2><h3 id="原文-5"><a href="#原文-5" class="headerlink" title="原文"></a>原文</h3><p>​    The heap managed by TCMalloc consists of a set of pages. A run of contiguous pages is represented by a <code>Span</code> object. A span can either be <em>allocated</em>, or <em>free</em>. If free, the span is one of the entries in a page heap linked-list. If allocated, it is either a large object that has been handed off to the application, or a run of pages that have been split up into a sequence of small objects. If split into small objects, the size-class of the objects is recorded in the span.</p><p>​    A central array indexed by page number can be used to find the span to which a page belongs. For example, span <em>a</em> below occupies 2 pages, span <em>b</em> occupies 1 page, span <em>c</em> occupies 5 pages and span <em>d</em> occupies 3 pages.</p><p><img src="/img/trans/spanmap.gif"></p><p>​    A 32-bit address space can fit 2^20 4K pages, so this central array takes 4MB of space, which seems acceptable. On 64-bit machines, we use a 3-level radix tree instead of an array to map from a page number to the corresponding span pointer.</p><h3 id="译-5"><a href="#译-5" class="headerlink" title="译"></a>译</h3><p>​    TCMalloc 管理的堆由一系列的页组成。一系列连续的页可以用一个span对象来表示。一个span既可以被分配，也可以被释放。</p><ul><li>如果空闲，则span 是页堆链表中的条目之一。</li><li>如果已分配，它要么是已移交给应用程序的大对象，要么是已拆分为一系列小对象的连续页。</li><li>如果拆分为小对象，则将对象的size-class记录在span中。</li></ul><p>​    以页码为索引的中央数组可用于查找页所属的span。比如下面的span <em>a</em>占2页，span <em>b</em>占1页，span <em>c</em>占5页，span <em>d</em>占3页。</p><p>​    一个 32 位的地址空间可以容纳 2^20 个 4K 的页，所以这个中央数组需要 4MB 的空间，这似乎是可以接受的。在 64 位机器上，我们使用 3 级基数树代替数组来从页码映射到相应的span指针。</p><h2 id="解除分配"><a href="#解除分配" class="headerlink" title="解除分配"></a>解除分配</h2><h3 id="原文-6"><a href="#原文-6" class="headerlink" title="原文"></a>原文</h3><p>​    When an object is deallocated, we compute its page number and look it up in the central array to find the corresponding span object. The span tells us whether or not the object is small, and its size-class if it is small. If the object is small, we insert it into the appropriate free list in the current thread’s thread cache. If the thread cache now exceeds a predetermined size (2MB by default), we run a garbage collector that moves unused objects from the thread cache into central free lists.</p><p>​    If the object is large, the span tells us the range of pages covered by the object. Suppose this range is <code>[p,q]</code>. We also lookup the spans for pages <code>p-1</code> and <code>q+1</code>. If either of these neighboring spans are free, we coalesce them with the <code>[p,q]</code> span. The resulting span is inserted into the appropriate free list in the page heap.</p><h3 id="译-6"><a href="#译-6" class="headerlink" title="译"></a>译</h3><p>​    当一个对象被释放时，我们计算它的页码并在中央数组中查找它以找到相应的span对象。span告诉我们是否是小对象，以及它的size-class是否是小。</p><ul><li><p>如果释放的是小对象，那么我们就将它插入到当前线程的线程缓存中适当的空闲队列中。如果线程缓存现在超过预订大小（默认2MB）,我们将运行垃圾收集器，将未使用的对象从线程缓存移动到中央空闲列表中。</p></li><li><p> 如果释放的是大对象，span告诉我们对象覆盖的范围。假设这个范围是<code>[p,q]</code>。 我们还查找页“p-1”和“q+1”的spans。如果这些相邻spans中的任何一个是空闲的，我们将它们与“[p,q]” span合并。生成的span被插入到页堆中适当的空闲列表中。</p></li></ul><h2 id="小对象的中央空闲列表"><a href="#小对象的中央空闲列表" class="headerlink" title="小对象的中央空闲列表"></a>小对象的中央空闲列表</h2><h3 id="原文-7"><a href="#原文-7" class="headerlink" title="原文"></a>原文</h3><p>​    As mentioned before, we keep a central free list for each size-class. Each central free list is organized as a two-level data structure: a set of spans, and a linked list of free objects per span.</p><p>​    An object is allocated from a central free list by removing the first entry from the linked list of some span. (If all spans have empty linked lists, a suitably sized span is first allocated from the central page heap.)</p><p>​    An object is returned to a central free list by adding it to the linked list of its containing span. If the linked list length now equals the total number of small objects in the span, this span is now completely free and is returned to the page heap.</p><h3 id="译-7"><a href="#译-7" class="headerlink" title="译"></a>译</h3><p>​    如前所述，我们为每个size-class保留一个中央空闲列表。 每个中央空闲列表被组织为一个两级数据结构：一组span，以及每个span的空闲对象链表。</p><p>​    通过从某个span的链表中删除第一个条目，从中央空闲列表中分配对象。 （如果所有span都有空链表，则首先从中央页堆中分配一个合适大小的span。）</p><p>​    通过将对象添加到span包含的链表中,，可以将其返回到中央空闲列表。 如果链表长度现在等于span中小对象的总数，那么这个span现在完全空闲并返回到页堆。</p><h2 id="线程缓存的垃圾收集"><a href="#线程缓存的垃圾收集" class="headerlink" title="线程缓存的垃圾收集"></a>线程缓存的垃圾收集</h2><h3 id="原文-8"><a href="#原文-8" class="headerlink" title="原文"></a>原文</h3><p>​    A thread cache is garbage collected when the combined size of all objects in the cache exceeds 2MB. The garbage collection threshold is automatically decreased as the number of threads increases so that we don’t waste an inordinate amount of memory in a program with lots of threads.</p><p>​    We walk over all free lists in the cache and move some number of objects from the free list to the corresponding central list.</p><p>​    The number of objects to be moved from a free list is determined using a per-list low-water-mark <code>L</code>. <code>L</code> records the minimum length of the list since the last garbage collection. Note that we could have shortened the list by <code>L</code> objects at the last garbage collection without requiring any extra accesses to the central list. We use this past history as a predictor of future accesses and move <code>L/2</code> objects from the thread cache free list to the corresponding central free list. This algorithm has the nice property that if a thread stops using a particular size, all objects of that size will quickly move from the thread cache to the central free list where they can be used by other threads.</p><h3 id="译-8"><a href="#译-8" class="headerlink" title="译"></a>译</h3><p>​    当缓存中所有对象的总大小超过 2MB 时，线程缓存将被垃圾回收。随着线程数量的增加，垃圾收集阈值会自动降低，这样我们就不会在具有大量线程的程序中浪费过多的内存。</p><p>​    我们遍历缓存中的所有空闲列表，并将一定数量的对象从空闲列表移动到相应的中央列表。</p><p>​    每个链表的低水位标记L决定了从空闲链中移出对象的数量。L记录了自从上一次垃圾收集操作之后本链表的最小长度。注意我们可以缩短链的长度，通过在前一次垃圾收集时移走L个对象，并且没有从中央链中获取其他对象。我们使用这个过去的记录来预测未来的情况，从线程缓存中移走L/2个对象到中央链表中。这个算法性能良好，如果一个线程停止使用某个特定大小的对象，该大小的所有对象将会很快的从线程缓存中迁移到中央空闲链中，以便被其他线程来使用。</p><h2 id="性能说明"><a href="#性能说明" class="headerlink" title="性能说明"></a>性能说明</h2><h3 id="PTMalloc2单元测试"><a href="#PTMalloc2单元测试" class="headerlink" title="PTMalloc2单元测试"></a>PTMalloc2单元测试</h3><h3 id="原文-9"><a href="#原文-9" class="headerlink" title="原文"></a>原文</h3><p>​    The PTMalloc2 package (now part of glibc) contains a unittest program t-test1.c. This forks a number of threads and performs a series of allocations and deallocations in each thread; the threads do not communicate other than by synchronization in the memory allocator.</p><p>​    t-test1 (included in google-perftools/tests/tcmalloc, and compiled as ptmalloc_unittest1) was run with a varying numbers of threads (1-20) and maximum allocation sizes (64 bytes - 32Kbytes). These tests were run on a 2.4GHz dual Xeon system with hyper-threading enabled, using Linux glibc-2.3.2 from RedHat 9, with one million operations per thread in each test. In each case, the test was run once normally, and once with LD_PRELOAD=libtcmalloc.so.</p><p>​    The graphs below show the performance of TCMalloc vs PTMalloc2 for several different metrics. Firstly, total operations (millions) per elapsed second vs max allocation size, for varying numbers of threads. The raw data used to generate these graphs (the output of the “time” utility) is available in t-test1.times.txt.</p><table><thead><tr><th><img src="/img/trans/tcmalloc-opspersec.vs.size.1.threads.png"></th><th><img src="/img/trans/tcmalloc-opspersec.vs.size.2.threads.png" alt="img"></th><th><img src="/img/trans/tcmalloc-opspersec.vs.size.3.threads.png" alt="img"></th></tr></thead><tbody><tr><td><img src="/img/trans/tcmalloc-opspersec.vs.size.4.threads.png" alt="img"></td><td><img src="/img/trans/tcmalloc-opspersec.vs.size.5.threads.png" alt="img"></td><td><img src="/img/trans/tcmalloc-opspersec.vs.size.8.threads.png" alt="img"></td></tr><tr><td><img src="/img/trans/tcmalloc-opspersec.vs.size.12.threads.png" alt="img"></td><td><img src="/img/trans/tcmalloc-opspersec.vs.size.16.threads.png" alt="img"></td><td><img src="/img/trans/tcmalloc-opspersec.vs.size.20.threads.png" alt="img"></td></tr></tbody></table><ul><li><p>TCMalloc is much more consistently scalable than PTMalloc2 - for all thread counts &gt;1 it achieves ~7-9 million ops/sec for small allocations, falling to ~2 million ops/sec for larger allocations. The single-thread case is an obvious outlier, since it is only able to keep a single processor busy and hence can achieve fewer ops/sec. PTMalloc2 has a much higher variance on operations/sec - peaking somewhere around 4 million ops/sec for small allocations and falling to &lt;1 million ops/sec for larger allocations.</p></li><li><p>TCMalloc is faster than PTMalloc2 in the vast majority of cases, and particularly for small allocations. Contention between threads is less of a problem in TCMalloc.</p></li><li><p>TCMalloc’s performance drops off as the allocation size increases. This is because the per-thread cache is garbage-collected when it hits a threshold (defaulting to 2MB). With larger allocation sizes, fewer objects can be stored in the cache before it is garbage-collected.</p></li><li><p>There is a noticeably drop in the TCMalloc performance at ~32K maximum allocation size; at larger sizes performance drops less quickly. This is due to the 32K maximum size of objects in the per-thread caches; for objects larger than this tcmalloc allocates from the central page heap.</p><p>Next, operations (millions) per second of CPU time vs number of threads, for max allocation size 64 bytes - 128 Kbytes.</p></li></ul><table><thead><tr><th><img src="/img/trans/tcmalloc-opspercpusec.vs.threads.64.bytes.png" alt="img"></th><th><img src="/img/trans/tcmalloc-opspercpusec.vs.threads.256.bytes.png" alt="img"></th><th><img src="/img/trans/tcmalloc-opspercpusec.vs.threads.1024.bytes.png" alt="img"></th></tr></thead><tbody><tr><td><img src="/img/trans/tcmalloc-opspercpusec.vs.threads.4096.bytes.png" alt="img"></td><td><img src="/img/trans/tcmalloc-opspercpusec.vs.threads.8192.bytes.png" alt="img"></td><td><img src="/img/trans/tcmalloc-opspercpusec.vs.threads.16384.bytes.png" alt="img"></td></tr><tr><td><img src="/img/trans/tcmalloc-opspercpusec.vs.threads.32768.bytes.png" alt="img"></td><td><img src="/img/trans/tcmalloc-opspercpusec.vs.threads.65536.bytes.png" alt="img"></td><td><img src="/img/trans/tcmalloc-opspercpusec.vs.threads.131072.bytes.png" alt="img"></td></tr></tbody></table><p>​    Here we see again that TCMalloc is both more consistent and more efficient than PTMalloc2. For max allocation sizes &lt;32K, TCMalloc typically achieves ~2-2.5 million ops per second of CPU time with a large number of threads, whereas PTMalloc achieves generally 0.5-1 million ops per second of CPU time, with a lot of cases achieving much less than this figure. Above 32K max allocation size, TCMalloc drops to 1-1.5 million ops per second of CPU time, and PTMalloc drops almost to zero for large numbers of threads (i.e. with PTMalloc, lots of CPU time is being burned spinning waiting for locks in the heavily multi-threaded case).</p><h3 id="译-9"><a href="#译-9" class="headerlink" title="译"></a>译</h3><p>​    PTMalloc2包（现在是glibc的一部分）包含一个单元测试程序 t-test1.c。这会派生多个线程并在每个线程中执行一系列分配和释放；除了通过内存分配器中的同步之外，线程不进行通信。</p><p>​    t-test1（包含在 google-perftools/tests/tcmalloc 中，并编译为 ptmalloc_unittest1）以不同数量的线程（1-20）和最大分配大小（64 字节 - 32 KB）运行。这些测试在启用了超线程的 2.4GHz 双 Xeon 系统上运行，使用来自 RedHat 9 的 Linux glibc-2.3.2，每个测试中每个线程有 100 万次操作。在每种情况下，测试都正常运行一次，并使用 LD_PRELOAD=libtcmalloc.so 运行一次。</p><p>​    下图显示了 TCMalloc 与 PTMalloc2 在几个不同指标下的性能。首先，对于不同数量的线程，每秒总操作数（百万）与最大分配大小。用于生成这些图形的原始数据（“时间”实用程序的输出）在 t-test1.times.txt 中可用。</p><ul><li>TCMalloc 比 PTMalloc2 具有更高的可扩展性 - 对于所有线程数大于 1 的情况，小分配实现约 7-9 百万次操作/秒，较大分配时降至约 200 万次操作/秒。单线程情况是一个明显的异常值，因为它只能保持单个处理器忙碌，因此可以实现更少的操作/秒。 PTMalloc2 在操作/秒上的差异要大得多——小分配的峰值约为 400 万次操作/秒，而较大的分配则下降到 &lt;100 万次操作/秒。</li><li>在绝大多数情况下，TCMalloc 比 PTMalloc2 快，特别是对于小分配。线程之间的争用在 TCMalloc 中不是问题。</li><li>TCMalloc 的性能随着分配大小的增加而下降。这是因为每线程缓存在达到阈值（默认为 2MB）时会被垃圾收集。使用更大的分配大小，在缓存被垃圾收集之前可以存储在缓存中的对象更少。</li><li>TCMalloc 性能在 ~32K 最大分配大小时显着下降；在较大的尺寸下，性能下降的速度较慢。这是由于每个线程缓存中对象的最大大小为 32K；对于大于此 tcmalloc 从中央页堆分配的对象。</li></ul><p>​    接下来，CPU 时间每秒的操作数（百万）与线程数的关系，最大分配大小为 64 字节 - 128 KB。</p><p>​    在这里我们再次看到 TCMalloc 比 PTMalloc2 更一致、更高效。 对于最大分配大小小于 32K 的情况，TCMalloc 通常在大量线程的情况下实现每秒约 2-250 万次操作，而 PTMalloc 通常实现每秒 0.5-100 万次操作，在很多情况下实现了很多 低于这个数字。 超过 32K 的最大分配大小，TCMalloc 的 CPU 时间下降到每秒 1-150 万次操作，对于大量线程，PTMalloc 几乎下降到零（即使用 PTMalloc，大量 CPU 时间正在消耗大量 CPU 时间等待锁 多线程情况）。</p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><h3 id="原文-10"><a href="#原文-10" class="headerlink" title="原文"></a>原文</h3><p>​    For some systems, TCMalloc may not work correctly on with applications that aren’t linked against libpthread.so (or the equivalent on your OS). It should work on Linux using glibc 2.3, but other OS/libc combinations have not been tested.</p><p>​    TCMalloc may be somewhat more memory hungry than other mallocs, though it tends not to have the huge blowups that can happen with other mallocs. In particular, at startup TCMalloc allocates approximately 6 MB of memory. It would be easy to roll a specialized version that trades a little bit of speed for more space efficiency.</p><p>​    TCMalloc currently does not return any memory to the system.</p><p>​    Don’t try to load TCMalloc into a running binary (e.g., using JNI in Java programs). The binary will have allocated some objects using the system malloc, and may try to pass them to TCMalloc for deallocation. TCMalloc will not be able to handle such objects.</p><h3 id="译-10"><a href="#译-10" class="headerlink" title="译"></a>译</h3><p>​    对于某些系统，TCMalloc可能无法在不与libpthread.so相关联的应用程序上正常工作。它应该工作在使用glibc 2.3的Linux上，但其他的 OS/libc组合还没有经过测试。</p><p>​    TCMalloc可能比其他mallocs更需要内存，尽管它不会发生其他mallocs可能发生的巨大爆炸。特别是在启动时，TCMalloc需分配6MB的内存。推出一个专门的版本会很容易，该版本以一点速度换取更多空间效率。</p><p>​    TCMalloc 目前不会向系统返回任何内存。</p><p>​    不要尝试将 TCMalloc 加载到正在运行的二进制文件中（例如，在 Java 程序中使用 JNI）。 二进制文件将使用系统 malloc 分配一些对象，并可能尝试将它们传递给 TCMalloc 进行释放。 TCMalloc 将无法处理此类对象。</p><h3 id="原文地址"><a href="#原文地址" class="headerlink" title="原文地址"></a>原文地址</h3><p><a href="http://goog-perftools.sourceforge.net/doc/tcmalloc.html">TCMalloc : Thread-Caching Malloc</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Golang </tag>
            
            <tag> TCMalloc </tag>
            
            <tag> GC </tag>
            
            <tag> 译文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gRPC长期流式传输【译】</title>
      <link href="/2021/05/09/trans/grpc-long-lived-streaming-trans/"/>
      <url>/2021/05/09/trans/grpc-long-lived-streaming-trans/</url>
      
        <content type="html"><![CDATA[<p><img src="/img/trans/grpc-1.png"></p><h6 id="预计阅读时间：13分钟"><a href="#预计阅读时间：13分钟" class="headerlink" title="预计阅读时间：13分钟"></a>预计阅读时间：13分钟</h6><p>​    在这篇博客文章中，我将探讨一种实现gRPC长期流式传输的方法。使用gRPC非常适合云原生应用程序，这主要是因为它很现代，带宽和CPU效率以及低延迟，这正是分布式系统所需的。</p><p>​    如果您正在阅读本文，我认为您已经熟悉gRPC。 但是，如果您仍然觉得需要介绍，请在下面留下评论，我也将整理一份gRPC入门文章。</p><h3 id="什么被称为“长”-RPC会话？"><a href="#什么被称为“长”-RPC会话？" class="headerlink" title="什么被称为“长” RPC会话？"></a>什么被称为“长” RPC会话？</h3><p>​    典型的RPC是立即请求-响应机制。 当提到长期存在的RPC时，你们中的某些人可能会想到不同的时间范围。</p><p>​    长期存在的RPC通常要比常规RPC的调用大一个数量级。例如，它可以持续数分钟，数小时，数天等，具体取决于您的用例。</p><p>​    在深入研究实现之前，我们首先考虑长期使用的RPC流的用例：</p><h3 id="长期的RPC用例"><a href="#长期的RPC用例" class="headerlink" title="长期的RPC用例"></a>长期的RPC用例</h3><p>​    现在，让我们重点介绍长期存在的RPC的一些主要用例。其中一些对于同一用例而言可能是不同的名称，但我想确保涵盖最常见的名称：</p><ul><li><p><strong>Watch API</strong> - 修改API对象后，您要在其中获取通知的方式（类似于Kubernetes监视API机制）。</p></li><li><p><strong>Notifications</strong>  - 发生某些后端事件时，您希望立即收到有关该事件的通知。</p></li><li><p><strong>Subscribers</strong>  - 几种服务可以订阅事件并立即接收事件。 该用例还可以包括退订功能。</p><p>请注意，我上面提到的所有用例都可以通过使用轮询来解决。 但是，如果您正在阅读本文，我想您应该避免轮询。 通过使用长寿命的流，您可以立即做出响应并减少事件的延迟。 可以将RPC的使用视为“管道”，它已设置并准备在任何给定时间处理事件。</p></li></ul><h3 id="gRPC故障处理"><a href="#gRPC故障处理" class="headerlink" title="gRPC故障处理"></a>gRPC故障处理</h3><p>​    使用gRPC的好处是它可以控制一些机制来帮助您处理故障。 其中一些是：</p><ul><li><a href="https://github.com/grpc/grpc/blob/master/doc/connection-backoff.md">连接回退</a> - 当我们与失败的后端进行连接时，通常不希望立即重试（以避免用请求淹没网络或服务器），而是执行某种形式的指数回退。</li><li><a href="https://github.com/grpc/grpc/blob/master/doc/keepalive.md">Keepalive</a> - Keepalive ping是一种通过在传输上发送HTTP2 ping来检查通道当前是否正在工作的方法。 它是定期发送的，如果在一定的超时时间内对等方未确认ping，则传输断开。</li></ul><p>​    这并不意味着您不需要处理网络故障的情况。 在设计生产等级系统时，您绝对应该考虑到这一点。</p><h3 id="我们要建造什么？"><a href="#我们要建造什么？" class="headerlink" title="我们要建造什么？"></a>我们要建造什么？</h3><p>​    以下GitHub存储库中提供了此处使用的所有代码：</p><p>​    <code>https://github.com/omri86/longlived-grpc</code></p><p>​    为了使一切保持简单，并专注于如何使用gRPC，我们将创建一个由单个服务器和多个客户端组成的基本应用程序。 我选择与任意数量的10个客户端一起工作，但是正如您稍后将看到的，这很容易扩展：</p><p><img src="/img/trans/grpc.png"></p><h3 id="应用流程"><a href="#应用流程" class="headerlink" title="应用流程"></a>应用流程</h3><p>​    以下是构建此应用程序时我想到的一般流程。 注意，<strong>顺序不是固定的</strong>。 例如，客户端可以在服务器启动之前启动。</p><ol><li>服务器启动并等待客户端订阅</li><li>客户端启动并向服务器发送订阅请求</li><li>服务器订阅客户端</li><li>服务器定期向客户端发送数据</li></ol><p><img src="/img/trans/grpc2.png"></p><p>一些注意事项：</p><ul><li>如上所述，我们将有几个客户端，因此服务器将向每个客户端发送数据。</li><li>发送数据可以基于事件。 为了简单起见，我选择定期执行此操作。</li><li>每个组件都可以优雅地处理错误-稍后会详细介绍。</li></ul><h3 id="创建gRPC-proto文件"><a href="#创建gRPC-proto文件" class="headerlink" title="创建gRPC proto文件"></a>创建gRPC proto文件</h3><p>​    在此处查看完整的proto文件：<a href="https://github.com/omri86/longlived-grpc/blob/master/protos/longlived.proto">longlived.proto</a></p><p>​    正如该帖子的标题所示，您将使用服务端流式RPC。 这是通过以下方式声明的：</p><pre><code class="protobuf">service Longlived &#123;  rpc Subscribe(Request) returns (stream Response) &#123;&#125;&#125;</code></pre><p>​    服务器将处理订阅请求（因此有Request参数），并将返回Response。 让我们看一下这两个对象：</p><pre><code class="protobuf">message Request &#123;  int32 id = 1;&#125;message Response &#123;  string data = 1;&#125;</code></pre><p>​    <code>Request</code>对象拥有一个ID，这将是客户端标识符。<code>Response</code>对象保存数据–这是您将从服务器发送到订阅的客户端的数据。</p><p>​    proto文件还包含一个“取消订阅”一元RPC。 此功能将不会用作本教程的一部分，但可以为您提供有关如何退订的示例：</p><pre><code class="protobuf">rpc Unsubscribe(Request) returns (Response) &#123;&#125;</code></pre><h3 id="创建服务端"><a href="#创建服务端" class="headerlink" title="创建服务端"></a>创建服务端</h3><p>​    在此处查看完整的服务端文件：<a href="https://github.com/omri86/longlived-grpc/blob/master/server/server.go">server.go</a></p><p>​    首先，让我们看一下服务端的结构体：</p><pre><code class="go">type longlivedServer struct &#123;     protos.UnimplementedLonglivedServer     subscribers sync.Map // subscribers is a concurrent map that holds mapping from a client ID to it&#39;s subscriber &#125; type sub struct &#123;     stream   protos.Longlived_SubscribeServer // stream is the server side of the RPC stream     finished chan&lt;- bool                      // finished is used to signal closure of a client subscribing goroutine &#125;</code></pre><ul><li>在<a href="https://github.com/grpc/grpc-go/issues/3669">该线程</a>和此<a href="https://github.com/grpc/grpc-go/blob/master/cmd/protoc-gen-go-grpc/README.md#future-proofing-services">README</a>文件中可以找到#2的解释。</li><li>订阅者结构体将保留每个订阅您的服务端的客户端。 它将保存从客户端ID到服务端流的映射，您将很快看到它的创建和用途。</li><li>由于服务器可以并行地将数据发送到订阅服务器并接收订阅请求，因此您需要确保没有冲突，这就是为什么需要并发映射的原因。</li><li><code>sub</code>结构体将保存为 map 值。 它代表一个拥有以下内容的订户：<ul><li>  服务端流</li><li>发出关闭流信号的通道</li></ul></li></ul><h3 id="服务端订阅方法"><a href="#服务端订阅方法" class="headerlink" title="服务端订阅方法"></a>服务端订阅方法</h3><p>​    为了实现在proto文件中定义的gRPC服务端接口，您需要实现以下方法：</p><pre><code class="go">func (s *longlivedServer) Subscribe(request *protos.Request, stream protos.Longlived_SubscribeServer) error</code></pre><p>​    此方法对来自客户端的每个传入订阅请求具有单独的上下文（专用<code>goroutine</code>）。 您将收到客户端请求和相应的流，该流用于将数据流传输到客户端。</p><p>​    关于此函数的重要说明是，<strong>一旦此函数返回，流将关闭</strong>。 只要订阅了客户端，此功能范围就必须保持活动状态。</p><h3 id="订阅客户端"><a href="#订阅客户端" class="headerlink" title="订阅客户端"></a>订阅客户端</h3><p>​    由于只要在单独的<code>goroutine</code>中订阅了客户端，此功能就会运行，因此您需要一种访问其流的方法，以便将数据发送到订阅的客户端。</p><p>​    您还需要一种方法来通知此<code>goroutine</code>在流关闭的情况下退出。</p><p>​    这就是为什么我们需要为每个客户流创建一个专用<code>channel</code>（<code>fin</code> channel）的原因。 我选择实现此机制的方法是将客户端ID映射到其对应的通道和流：</p><pre><code class="go">fin := make(chan bool)// Save the subscriber stream according to the given client IDs.subscribers.Store(request.Id, sub&#123;stream: stream, finished: fin&#125;)</code></pre><p>​    而且，如上所述，需要保护对此映射的写入（或从中读取），这就是为什么我使用了并发映射。</p><p>​    您要做的最后一件事是等待<code>channel</code>上2个可能的事件之一：</p><pre><code>1. 发生错误时，您将发送一条消息通知该通道，请关闭该通道（您将在下面使用此通道的另一侧）。2. 发送给`ctx.Done`的消息–这是客户端断开连接的通信方式。</code></pre><pre><code class="go">for &#123;   select &#123;   case &lt;-fin:      log.Printf(&quot;Closing stream for client ID: %d&quot;, request.Id)   case &lt;- ctx.Done():      log.Printf(&quot;Client ID %d has disconnected&quot;, request.Id)      return nil   &#125;&#125;</code></pre><h3 id="创建客户端"><a href="#创建客户端" class="headerlink" title="创建客户端"></a>创建客户端</h3><p>​    在此处查看完整的客户端代码：<a href="https://github.com/omri86/longlived-grpc/blob/master/client/client.go">client.go</a></p><p>​    首先让我们来看一下客户端结构体：</p><pre><code class="go">type longlivedClient struct &#123;   client protos.LonglivedClient // client is the long lived gRPC client   conn   *grpc.ClientConn       // conn is the client gRPC connection   id     int32                  // id is the client ID used for subscribing&#125;</code></pre><ul><li><code>client</code>代表gRPC客户端，我们将很快对其进行初始化。</li><li><code>conn</code>将保持gRPC连接（客户端&lt;-&gt;服务器）</li><li>如服务器端所述，客户端正在订阅其唯一ID。 id字段是保存此ID的字段。</li></ul><h3 id="客户端订阅方法"><a href="#客户端订阅方法" class="headerlink" title="客户端订阅方法"></a>客户端订阅方法</h3><p>​    为了订阅服务端更新，客户端必须调用gRPC <code>Subscribe()</code>函数。 这样做如下：</p><pre><code class="go">c.client.Subscribe(context.Background(), &amp;protos.Request&#123;Id: c.id&#125;)</code></pre><ul><li>可以将上下文设置为包含截止日期，取消信号等。 由于这不在本博客文章的讨论范围之内，因此您可以在<a href="https://golang.org/pkg/context/">此处</a>了解更多信息。</li><li>传递给服务器的第二个值是保存客户端<code>id</code>的客户端请求。</li></ul><h3 id="开启客户端"><a href="#开启客户端" class="headerlink" title="开启客户端"></a>开启客户端</h3><p>​    订阅后，该流用于将数据从服务器流式传输到客户端：</p><pre><code class="go">var stream protos.Longlived_SubscribeClient</code></pre><p>​    客户端应该做的第一件事是按照上一节中的说明进行订阅：</p><pre><code class="go">if stream == nil &#123;   if stream, err = c.subscribe(); err != nil &#123;      log.Printf(&quot;Failed to subscribe: %v&quot;, err)      c.sleep()      // Retry on failure      continue   &#125;&#125;</code></pre><p>​    如您所见，如果发生错误，我们只<code>sleep</code>几秒钟，然后尝试重新订阅。 这样做是为了确保客户端可以应对服务端器崩溃。 您需要确保客户端在服务器无响应的情况下继续重试连接。 稍后您将看到它如何工作。</p><p>​    客户端的下一部分和最后一部分是从流中接收数据：</p><pre><code class="go">response, err := stream.Recv()if err != nil &#123;   log.Printf(&quot;Failed to receive message: %v&quot;, err)   // Clearing the stream will force the client to resubscribe on next iteration   stream = nil   c.sleep()   // Retry on failure   continue&#125;log.Printf(&quot;Client ID %d got response: %q&quot;, c.id, response.Data)</code></pre><h3 id="创建实际的应用程序"><a href="#创建实际的应用程序" class="headerlink" title="创建实际的应用程序"></a>创建实际的应用程序</h3><p>​    现在基础已经明确，让我们运行我们的应用程序并查看其实际工作方式。</p><h3 id="服务端的主要方法"><a href="#服务端的主要方法" class="headerlink" title="服务端的主要方法"></a>服务端的主要方法</h3><p>​    您需要做的第一件事是初始化服务端：</p><pre><code class="go">lis, err := net.Listen(&quot;tcp&quot;, &quot;127.0.0.1:7070&quot;)if err != nil &#123;   log.Fatalf(&quot;failed to listen: %v&quot;, err)grpcServer := grpc.NewServer([]grpc.ServerOption&#123;&#125;...)server := &amp;longlivedServer&#123;&#125;</code></pre><p>​    下一步是运行后台任务（goroutine）以生成数据并将其发送到订阅客户端：</p><pre><code class="go">// Start sending data to subscribersgo server.mockDataGenerator()</code></pre><p>​    此函数遍历订阅的客户端，并在其相应的流上发送数据：</p><pre><code class="go">func (s *longlivedServer) mockDataGenerator() &#123;   log.Println(&quot;Starting data generation&quot;)   for &#123;      time.Sleep(time.Second)      // A list of clients to unsubscribe in case of error      var unsubscribe []int32      // Iterate over all subscribers and send data to each client      s.subscribers.Range(func(k, v interface&#123;&#125;) bool &#123;         id, ok := k.(int32)         if !ok &#123;            log.Printf(&quot;Failed to cast subscriber key: %T&quot;, k)            return false         &#125;         sub, ok := v.(sub)         if !ok &#123;            log.Printf(&quot;Failed to cast subscriber value: %T&quot;, v)            return false         &#125;         // Send data over the gRPC stream to the client         if err := sub.stream.Send(&amp;protos.Response&#123;Data: fmt.Sprintf(&quot;data mock for: %d&quot;, id)&#125;); err != nil &#123;            log.Printf(&quot;Failed to send data to client: %v&quot;, err)            select &#123;            case sub.finished &lt;- true:               log.Printf(&quot;Unsubscribed client: %d&quot;, id)            default:               // Default case is to avoid blocking in case client has already unsubscribed            &#125;            // In case of error the client would re-subscribe so close the subscriber stream            unsubscribe = append(unsubscribe, id)         &#125;         return true      &#125;)      // Unsubscribe erroneous client streams      for _, id := range unsubscribe &#123;         s.subscribers.Delete(id)      &#125;   &#125;&#125;</code></pre><p>​    剩下的就是注册并启动服务端：</p><pre><code class="go">// Register the serverprotos.RegisterLonglivedServer(grpcServer, server)log.Printf(&quot;Starting server on address %s&quot;, lis.Addr().String())// Start listeningif err := grpcServer.Serve(lis); err != nil &#123;   log.Fatalf(&quot;failed to listen: %v&quot;, err)&#125;</code></pre><p>​    您可以立即启动服务端：</p><pre><code class="go">$ go build server.go$ ./server2021/03/04 08:48:09 Starting server on address 127.0.0.1:70702021/03/04 08:48:09 Starting data generation</code></pre><h3 id="客户端的主要方法"><a href="#客户端的主要方法" class="headerlink" title="客户端的主要方法"></a>客户端的主要方法</h3><p>​    对于客户端，您将在同一过程中模拟多个客户端。 这可以轻松地单独完成：</p><pre><code class="go">func main() &#123;   // Create multiple clients and start receiving data   var wg sync.WaitGroup   for i := 1; i &lt;= 10; i++ &#123;      wg.Add(1)      client, err := mkLonglivedClient(int32(i))      if err != nil &#123;         log.Fatal(err)      &#125;      go client.start()      time.Sleep(time.Second*2)   &#125;   // The wait group purpose is to avoid exiting, the clients do not exit   wg.Wait()&#125;</code></pre><p>​    服务器已经启动并且正在运行，因此让我们运行客户端：</p><pre><code class="go">$ go build client.go$ ./client2021/03/04 09:19:29 Subscribing client ID: 1 2021/03/04 09:19:29 Client ID 1 got response: &quot;data mock for: 1&quot; 2021/03/04 09:19:30 Client ID 1 got response: &quot;data mock for: 1&quot; 2021/03/04 09:19:31 Subscribing client ID: 2 2021/03/04 09:19:31 Client ID 1 got response: &quot;data mock for: 1&quot; 2021/03/04 09:19:31 Client ID 2 got response: &quot;data mock for: 2&quot; 2021/03/04 09:19:32 Client ID 1 got response: &quot;data mock for: 1&quot; 2021/03/04 09:19:32 Client ID 2 got response: &quot;data mock for: 2&quot; 2021/03/04 09:19:33 Subscribing client ID: 3 2021/03/04 09:19:33 Client ID 2 got response: &quot;data mock for: 2&quot; 2021/03/04 09:19:33 Client ID 1 got response: &quot;data mock for: 1&quot; 2021/03/04 09:19:33 Client ID 3 got response: &quot;data mock for: 3&quot; 2021/03/04 09:19:34 Client ID 1 got response: &quot;data mock for: 1&quot; 2021/03/04 09:19:34 Client ID 3 got response: &quot;data mock for: 3&quot; 2021/03/04 09:19:34 Client ID 2 got response: &quot;data mock for: 2&quot; 2021/03/04 09:19:35 Subscribing client ID: 4 2021/03/04 09:19:35 Client ID 2 got response: &quot;data mock for: 2&quot; 2021/03/04 09:19:35 Client ID 4 got response: &quot;data mock for: 4&quot;</code></pre><p>​    如您所见，客户端正在启动，正在按预期方式订阅和接收数据。</p><h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><p>​    客户端和服务器都可以通过简单地重试连接来处理另一端的错误。</p><p>​    让我们通过停止服务器并查看客户端日志来进行测试：</p><pre><code class="go">2021/03/07 19:38:43 Failed to receive message: rpc error: code = Unavailable desc = transport is closing2021/03/07 19:38:48 Failed to subscribe: rpc error: code = Unavailable desc = connection error: desc = &quot;transport: Error while dialing dial tcp 127.0.0.1:7070: connect: connection refused&quot;</code></pre><p>​    如您所见，由于传输流正在关闭，每个客户端都会首先收到RPC错误。 由于服务器未响应，因此导致错误未能订阅–因此发生了传输错误：<code>connect：连接被拒绝</code>。</p><p>​    现在，再次启动服务器，并停止客户端–让我们查看服务器日志：</p><pre><code class="go">2021/03/07 19:43:04 Failed to send data to client: rpc error: code = Unavailable desc = transport is closing</code></pre><p>​    与客户端相同的错误，但是由于我们删除了已订阅的客户端并重试，因此不会收到其他任何错误。</p><p>​    再次启动客户端将重新订阅客户端，并且您可以看到客户端和服务端都按预期工作。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>​    起初，对于长期流使用gRPC可能会有些令人生畏，但是从本示例中可以看出–不必一定如此！</p><p>​    gRPC项目非常适合云原生应用程序，并且具有出色的社区和文档。</p><h3 id="原文链接"><a href="#原文链接" class="headerlink" title="原文链接"></a>原文链接</h3><p><a href="https://dev.bitolog.com/grpc-long-lived-streaming/">gRPC Long-lived Streaming</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Golang </tag>
            
            <tag> 译文 </tag>
            
            <tag> gRPC </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
